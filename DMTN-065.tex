\documentclass[DM,lsstdoc,toc]{lsstdoc}
\usepackage{graphicx}
\usepackage{url}
\usepackage{latexsym}
% \usepackage{color}
% black, blue, brown, cyan, darkgray, gray, green, lightgray, lime, magenta, blue, orange, pink, purple, red, teal, violet, white, yellow.
\usepackage{enumitem}

\title[LSST Special Programs]{Data Management \\ and LSST Special Programs}

\author{M.~L.~Graham, M.~Juri\'{c}, K.-T.~Lim, E.~Bellm, G.~Dubois-Felsmann, L.~Guy, and the Data Management System Science Team}

\setDocRef{DMTN-065}
\date{\today}
\setDocUpstreamLocation{\url{https://github.com/lsst-dm/dmtn-065}}

\setDocAbstract{
The core LSST science goals will be met by the Wide-Fast-Deep (WFD) Main Survey, which is expected to be accomplished with 85--90\% of the observing time.
The remaining 10--15\% of the time will be spent on Special Programs: additional survey areas and/or observing strategies that are driven by specific science goals which build on, or are beyond, the core science pillars of the LSST.

\medskip
This document provides a summary of the Special Programs being considered, the potential diversity of data they might produce (e.g., shorter or longer exposure times), and the role of Rubin Observatory in reducing, processing, and serving data products from Special Programs.
The latter includes a discussion about the relevant system requirements, the needed capabilities of the LSST Science Pipelines and the Rubin Science Platform, and the cases in which Special Programs data products would be user-generated.

\medskip
The specialized, science-specific aspects of Special Programs processing and analysis that are best left to the science community are also described, and illustrated with case-study examples.

\medskip
The main target audience of this document is Rubin Observatory staff -- the construction-era Data Management (DM) team and the operations-era Data Production (RDP) team -- but members of the science community who are planning to use Special Programs to reach their science goals may also find this document useful.
}

\setDocChangeRecord{%
\addtohist{0}{2017-11-14}{Status: internal working document.}{Melissa Graham}
\addtohist{1}{2018-06-17}{Updated to finalize and issue.}{Melissa Graham}
\addtohist{2}{2021-12-01}{Updates per DM-20375.}{Melissa Graham}
}

\begin{document}

\maketitle

% CITATION EXAMPLES
% \verb|\citellp|: \citellp{LPM-17, LSE-30} \\
% \verb|\citell|: (SRD; \citell{LPM-17,LSE-29}) \\
% \verb|\citep[][]|: \citep[e.g.,][are interesting]{LPM-17,LSE-29} \\
% \verb|\cite|: \cite{LPM-17,LSE-29}

% % % % % % % % % % % % % % % % % % 
\section{Terms and Definitions} \label{sec:terms}

\subsection{Wide-Fast-Deep (WFD) Main Survey}\label{ssec:terms_wfd}
The WFD Main Survey is the core science program of the LSST, designed to achieve the science goals defined by the Science Requirements Document (SRD; \citeds{LPM-17}).
The WFD will cover at least $\sim$18000 deg$^2$ and is expected to be accomplished with 85--90\% of the observing time.
As of the Phase 1 SCOC recommendations in \citeds{PSTN-053}, the WFD Main Survey is thought of as several contiguous areas.
The largest is the low-dust-extinction survey area, which would also receive the most ($>$800) visits over the 10-year survey and be completed to uniform depth, likely with a rolling cadence.
Smaller regions cover the higher-extinction high stellar density Galactic Bulge and Magellanic Clouds, the North Ecliptic Spur (NES), and the remainder of the Galactic Plane and the South Celestial Pole.
Boundaries, re-visit cadences, and depths for all WFD areas remain to be determined \citedsp{PSTN-053}.

\subsubsection{Mini-Surveys}\label{sssec:terms_wfd_mini}
As of the Phase 1 SCOC recommendations in \citeds{PSTN-053}, the term ``mini-survey" was being used to refer to the areas of the WFD / Main Survey outside of the large, low-dust-extinction survey area.
This term used to refer to non-WFD regions (Special Programs).

\subsection{Special Programs}\label{ssec:terms_sp}
Special Programs are additional survey areas and/or observing strategies that are driven by specific science goals which either build on, or are outside of, the core science pillars of the LSST.
Special Programs will fill the remaining 10--15\% of the available survey time, and include both ``deep drilling" fields and ``micro-surveys".

\subsubsection{Deep Drilling Field (DDF)}\label{sssec:terms_sp_ddf}
A DDF is a single pointing for which many exposures are obtained during the night.
The five DDFs (below) are expected to take $\sim$5\% of the total available time.
Generally, the LSST observing strategy for a DDF is to obtain some or all of these exposures consecutively, and to maintain a high inter-night cadence over a short season (e.g., returning every two nights over four months, \citealt{2019ApJ...873..111I}) -- but the exact strategies and total time for the LSST DDFs remain to be determined \citedsp{PSTN-053}.

\begin{itemize}
\item Elias S1 (00:37:48, -44:00:00)
\item XMM-LSS (02:22:50, -04:45:00)
\item Extended Chandra Deep Field-South (03:32:30, -28:06:00)
\item COSMOS (10:00:24, +02:10:55)
\item Euclid Deep Field South  (04:04:58, -48:25:23)\footnote{\url{https://www.cosmos.esa.int/web/euclid/euclid-survey}}
\end{itemize}

\subsubsection{Micro-Surveys}\label{sssec:terms_sp_micro}
As of the Phase 1 SCOC recommendations in \citeds{PSTN-053}, the term ``micro-survey" refers to either new sky areas observed with a WFD-like survey, or sky areas within the WFD but observed with a specialized strategy.
Micro-surveys are expected to take up the remaining $\sim$10\% of the total available survey time over LSST's 10 years, and those which would use $>$0.3\% of the time and are currently being considered by the SCOC are listed below (see also \citeds{PSTN-053}).

\begin{itemize}
\item short-exposure twilight visits (for near-Sun and near-Earth objects; NEOs)
\item a static short-exposure (5 sec) map of the sky in $ugrizy$ in the first year (for calibration)
\item an extended short-exposure survey of the sky in $ugrizy$ (for transient detection and static-sky calibration)
\item target-of-opportunity (ToO) follow-up (to identify optical counterparts to gravitational wave sources)
\item coverage of the Roman microlensing bulge field (potentially as a DDF)
\item deeper $g$-band imaging of 10 local volume galaxies
\item a high-cadence survey of 2 fields in the SMC (for microlensing)
\item annual week-long surveys of the Carina nebula and surrounding star-forming regions
\item a limited-visit (i.e., shallow) ``northern stripe" survey to declination $<$+30 degrees
\item a survey of the Virgo cluster to WFD depth
\end{itemize}

Note that there remains some grey area between what is a mini-survey within the WFD, and what is a WFD-like micro-survey.
For example, the last two on the list above (the northern stripe and the Virgo cluster) could also be (or might be in the future) called WFD mini-surveys, and not Special Programs micro-surveys.

\subsection{Visit Types}\label{ssec:terms_visits}
A visit is an observation of a single pointing at a given time, of which there are three types as listed below.
As of the Phase 1 SCOC recommendations in \citeds{PSTN-053}, most WFD and DDF visits would be standard or alternative standard visits.
However, non-standard visits with longer exposure times are being considered, especially for $u$-bands, and several of the micro-surveys are considering shorter exposure times.
The full potential for diversity in Special Programs data is reviewed in Section~\ref{ssec:proc_datadiv}.

\begin{itemize}
\item Standard Visit -- Composed of $2\times15$ second exposures (commonly referred to as ``snaps").
\item Alternative Standard Visit -- Composed of a single $30$ second exposure.
\item Non-Standard Visit -- Any other exposure time(s) or number of snaps.
\end{itemize}



\clearpage
% % % % % % % % % % % % % % % % % % 
\section{Reduction and Processing for Special Programs Data Products}\label{sec:proc}


\subsection{Rubin Requirements Related to Special Programs}
% \lsrreq, \ossreq, \dmreq
% \reqparam

A detailed list of the requirements related to Special Programs is provided in Appendix~\ref{sec:docrev}.
The most important requirements are summarized below.

\subsubsection{Data Products}

Rubin Observatory and the LSST system (the observatory and the data management systems) are required to process Special Programs data to produce unique and separate data products ``whenever possible" (LSR-REQ-0121).

The term ``whenever possible" includes cases where the original or reconfigured versions of the LSST Science Pipelines can be run, and excludes cases where the development of new algorithms or the allocation of significant additional computational resources are required (LSR-REQ-0121).

The statement ``to produce unique and separate data products" typically refers to producing the same kinds of data products as will be generated by the Prompt and Data Release pipelines (processed visit images, coadded images, difference images, and catalogs of sources and objects for those images).

It is a requirement that the cumulative size of the Special Programs data products generated by Rubin Observatory be no more than $\sim$10\% the size of the Data Release data products (i.e., proportional to the fraction of survey time spent; LSR-REQ-0121).

It is a requirement that these Special Programs data products be distinct, and joinable with (in other words, they can be federated or cross-matched with) the Prompt and Data Release data products (DMS-REQ-0322).

The derivation of value-added data products, such as HiPS or MOC maps, for Special Programs remains an open question (DMS-REQ-0379, 0383).

\subsubsection{Metadata}

In order to support Special Programs processing, the LSST system is required to store metadata that includes program information for every raw image, such as identifiers for images obtained as part of the Main Survey or a Special Program (DMS-REQ-0068).

It is required that this metadata be sufficient for Special Programs to trigger their own real-time data processing recipes ``whenever possible" (DMS-REQ-0320), and be included in alert packets (DMS-REQ-0274).

\subsubsection{Processing}

It is a requirement that Special Programs processing with the Prompt pipeline (or a reconfigured version of it) is subject to the same timescales and latency constraints of 24 hours for the release of Prompt data products and 1 minute for the transmission of Alert packets (DMS-REQ-0344).

It is also a requirement that Special Programs processing be done on timescales intermediate to the Prompt and Data Release processing, ``whenever possible" and whenever necessary to enable the intended science goals of the Special Program (LSR-REQ-0032).

It is a requirement that the LSST system be able to process non-standard visits with short exposure times as low as 1 second, with a discussion note that such short exposures might have degraded image quality (LSR-REQ-0111).

Processing for Special Programs by Rubin Observatory is expected to use no more than $\sim$10\% of computational and storage capacity of the Rubin data processing cluster (i.e., proportional to the fraction of survey time spent and the size of the Rubin-processed Special Programs data products; Section 6 of the DPDD).

\subsubsection{User Processing}

In cases where the science goals of a Special Program require that new algorithms or software be developed, User-Generated pipelines and data products will be needed.

The 10\% of the total data processing capacity that Rubin Observatory is required to reserve for all User-Generated processing includes that applied by users to Special Programs data -- in other words, there is no additional capacity {\it for users} that will be reserved only for Special Programs data (LSR-REQ-0041).





\subsection{The Potential Diversity of Special Programs Data} \label{ssec:proc_datadiv}

As discussed in Section~\ref{sec:terms}, most of the Special Programs that are currently under consideration will use standard or alternative standard visits.
However, some are likely to require non-standard visits with shorter exposures (5 sec).
Furthermore, some are likely to acquire images with a significantly brighter sky background (e.g., twilight images) than most nighttime survey images.

The cadence and patterns of Special Programs might also differ from the WFD, such as long series of exposures obtained of the same field (e.g., DDFs), or a strategy optimized to find very fast-moving objects (e.g., NEOs).

It does not appear that any of the currently-proposed Special Programs are likely to violate boundaries imposed by the Rubin Observatory hardware, but there remain a few open questions about boundaries on data processing imposed by the LSST Science Pipelines.

\subsubsection{Hardware Boundaries}

Appendix~\ref{sec:hardbounds} lists all of the hardware boundaries that might constrain the potential diversity of Special Programs data.

The minimum exposure time is 1 second (stretch goal: 0.1 seconds), and there is currently a hardware boundary that limits the readout rate to 1 every 15 seconds.
This would affect the image acquisition rate and increase the overheads of the proposed short-exposure micro-surveys.

Hardware imposes no other boundaries on how data can be obtained, but Special Programs that request a high number of filter changes and/or long slews could be inefficient due to large overheads.

\subsubsection{Processing Boundaries}

Appendix~\ref{sec:procbounds} describes the boundaries on what types of visits can be processed and calibrated by the LSST Science Pipelines, which are designed to process standard (or alternative standard) visits.

Very short ($<$2 sec) exposures could be difficult to reduce due to an incompletely-formed PSF, and very short or very long ($>$150 sec) exposures could be difficult to calibrate due to having too few (or too few unsaturated) stars.

It is currently unclear whether images with very bright sky backgrounds (twilight images) can be processed with the LSST Science Pipelines, or whether user generated pipelines will be needed.

The full reduction and calibration of images obtained with non-sidereal tracking, in which the stars are streaked, is currently beyond the scope of the LSST Science Pipelines, and will require a user generated pipeline.


\subsection{Anticipated Rubin-Processed Special Programs Data Products}

{\bf The LSST Project's Role in Processing Special Programs Data -- } The formal requirements regarding LSST's role in processing Special Programs data are in \citeds{LSE-61}, and the following statements have been derived from those requirements. The LSST Project will not take formal responsibility for specialized data reduction algorithms needed to process data, including images taken in non-standard modes. The term "specialized algorithms" refers to software that is not already within scope of the LSST Data Management (DM) science pipelines, and may include, for example: difference imaging for short exposures in which the PSF is not well-formed, shift-and-stack for faint moving objects, or any software with computational needs that significantly surpass the processing budget per image (compared to the processing of a WFD image). The Project will incorporate Special Programs data into the Prompt and/or Data Release processing pipelines and data products of the WFD Main Survey, such as {\tt Alerts}, {\tt CoAdds}, or {\tt Source} and {\tt Object} catalogs (with appropriate flags; LSST data products are described in \citeds{LSE-163}), whenever this (1) can be accomplished with existing software, and (2) is scientifically beneficial to that data product. The Project will also reconfigure its pipelines to generate separate imaging and catalog data products for Special Programs, whenever this can be accomplished with existing software. Finally, the Project will enable user-generated processing via the Science Platform (\citeds{LSE-319}), which will provide software tools and computational resources for (re)processing LSST data.

Whether or not Special Programs images are incorporated into the WFD main survey's data products, it is anticipated that most of the Special Program's science goals will require (or benefit from) separate data products (i.e., CoAdds and/or catalogs). For this reason, LSST intends to reconfigure the DM's pipelines in order to generate unique and separate -- but joinable -- imaging and catalog products for Special Programs data, whenever possible. In this context, ``possible" means that no new algorithms need to be written and that an intensive amount of additional computational resources is not required for the processing.

In a ``possible" scenario, DM would assemble a pipeline from existing DM codes in order to process data associated with a given Special Program and build image and catalog products that meet the science needs of that particular program. For example, for a DDF SN survey (see Section \ref{ssec:SPCS_SNDDF}), existing DM codes would be used to: (1) make a deep template image from a certain time window, (2) process standard single visit images, (3) create a nightly CoAdd, (4) run difference imaging analysis, (5) run source detection on the difference images, and (6) create \texttt{DIASource} and \texttt{DIAObject} catalog equivalents (this example is also given in Section 6 of the \DPDD, \citedsp{LSE-163}). This type of reconfiguration would also be possible to create as a user-generated pipeline (Section \ref{ssec:dmplans_user}), but having these products provided by the Project ensures a consistent and verified level of quality. %, as well as access for all users to the processed Special Programs data products, which would only increase the scientific value of Special Programs data. --> MLG: data rights doc will probably state that the products for SP data that is only processed by user-generated pipelines will have to be served to all users

The above statements of intent are derived from the Data Management Subsystem Requirements document, \citeds{LSE-61}, which contains several requirements related to the processing of data from Special Programs (DMS-REQ-0069, 0320, 0321, 0322, and 0344). 
Ensuring that the work-hours needed to reconfigure and test the pipelines, to run them and to verify and validate the data products for public release (which may potentially be needed on intermediate timescales that do not coincide with the Prompt/Yearly timescales, e.g., monthly stacks of deep drilling fields), is the role of Operations.

DMS-REQ-0320 states that "it shall be possible for special programs to trigger their own data processing recipes".
A header keyword identifying an image as related to a Special Program will be included (e.g., the survey name), and that will be sufficient to send it to a dedicated processing pipeline. 


\subsection{Anticipated User-Generated Special Programs Data Products}\label{ssec:dmplans_user}

In cases where the science goals of a Special Program require specialized algorithms and cannot be achieved by reconfiguring DM's software, then user-generated pipelines will be needed.
Towards this end, LSST DM is making all of its software open-source, and preparing the Science Platform (\citeds{LSE-319}), through which users can access the tools and computational resources to assemble data processing pipelines to achieve their science goals (whether related to Special Programs data or not).
During Operations, there will be a method for the system to allocation processing resources in the case of over-subscription.

If the user-generated processing pipeline for Special Programs data requires requires significantly more computational resources than have been allocated -- where that allocation has been sized approximately, based on image processing for WFD main survey data (i.e., difference imaging, source detection, and/or stacking) -- then external computational resources may be necessary.
To support such external processing DM intends to make the data and its code base accessible to and exportable by users in the science community.
No user-generated pipeline may contribute Alerts to the Alert Stream. %, although a separate stream should be possible if the packet and transport formats are adopted (see also Section \ref{ssec:dmplans_user}).

It is furthermore expected that, over time, some user-designed pipelines might become ``adopted", installed and operated (and change controlled) by the LSST Operations team.
For both adopted and user-run code, whether they are for Special Programs or WFD survey data, the LSST DM team will encourage and facilitate data product databases that are built with the same schema as -- and can easily be joined with -- the tables of the Prompt and DRP data products.
An alternative option to ``adopted" code is ``adopted" data products: situations in which user-generated code is run externally, a data catalog is returned to LSST to be ingested, verified, and made public.



\subsection{Including Special Programs Data in the WFD Main Survey's Data Products}\label{ssec:dmplans_WFD}

The Project may incorporate Special Programs data into the WFD main survey's pipelines and data products whenever this is (1) possible and (2) scientifically beneficial. This will likely be at the discretion of the data quality assessment team during Operations. In the following three sections we project when and how Special Programs data might be incorporated into the pipelines and data products of the Prompt pipeline and the Alert stream (Section \ref{ssec:dmplans_prompt}) and the annual Data Release pipeline (Section \ref{ssec:dmplans_drp}).

\subsubsection{The Prompt Pipeline and Alert Generation}\label{ssec:dmplans_prompt}

It would be beneficial to transient science to include as many LSST images into the Difference Imaging Analysis (DIA) pipeline and the Alert Stream as possible. Only images that can be processed with the Prompt DIA pipeline with the $60$-second timeline can contribute to the Alert Stream. As discussed in Section \ref{ssec:proc_datadiv}, this might prohibit exposures shorter than $<15$ seconds and/or visit cadences shorter than $1$ per $\sim30$ seconds. This might also prohibit the inclusion of very crowded fields that require more computational resources. A field must have an LSST template image to be processed by the DIA pipeline, which would prohibit immediate Alerts from new survey fields. It should be possible to load and use an alternative template image (than what would be used for that field if and when it is covered by the WFD main survey) in the Prompt pipeline for fields from a Special Program. 

There may be a couple of issues encountered with Alerts from many consecutive visits of Deep Drilling fields. One is that, since the Alert contains the full record of all associated {\tt DIASources} from the past 12 months (\citeds{LSE-163}), for a Deep Drilling Field with significantly more visits over the year, the size of the Alert might become prohibitively large (TBD). Another is that the self-consistency of the \texttt{DIAObjects} catalog may suffer during consecutive visits of a single field. For example, the processing for image $2$ of a sequence would begin when the processing for image $1$ is only halfway complete. Any new {\tt DIASource} in image $1$ that cannot be associated (by coordinate) with an existing {\tt DIAObject} becomes a new {\tt DIAObject}. When this source is again detected in image $2$, another new {\tt DIAObject} would be created if the catalog has not yet been updated.

% \textbf{Templates:} The template images that are used in the Level 1 difference imaging pipeline will be built from the Level 2 DRP, and so the first factor affecting a Special Programs image's suitability for Level 1 is to be in a region of sky with an existing template. So long as there is a template, when the exposure time is equivalent to a WFD visit image, $\sim 30$ seconds, treating the image as Level 1 is going to be fine. When processing Special Programs data with the Level 1 pipeline, certain science cases might call for the capability to load and use a certain template that is different from the WFD template (i.e., built over a different timescale). K.-T. confirms that there is not enough memory allotted to store more than one template over the whole sky, but for sub-regions, storing and using an alternative template should be possible. This is not an issue limited to Special Programs, since during commissioning it is conceivable that multiple template versions. K.-T. also confirms that the capability for the processing pipeline to choose a given template based on the programID in the raw image metadata will exist.

% \textbf{Processing backup:} Are there options for short-term increases in parallel processing power at NCSA? (Such options might be needed anyway to process crowded fields with $>$10k \texttt{DIASources}.) The bandwidth needed to load templates at a faster rate is also a concern, since templates have twice as many pixels, if the data acquistion rate is surveying at twice the WFD main survey rate, that's a $4\times$ additional bandwidth load. However, at NCSA there should be ways to elastically change the amount of processing power available. This is also an issue for crowded fields (below).

% Special Programs are more likely to include crowded fields than the WFD main survey area. Due to the increased number of sources, the number of \texttt{DIASources} -- and therefore the number of Alerts -- increases as well. In turn, this increases the processing time and in some cases, may exceed the 60 second limit for Alert Production. A policy is needed on whether Alerts from crowded fields should be allowed a delay, or allowed to be incomplete. K.-T. reports that the control system can easily kill processes that are running over time and move forward with existing outputs -- but that perhaps it will be just as easy to let it keep running and elastically grab additional NCSA resources as needed. This appears possible because the batch system is larger than the Level 1 allocation, although we might not know that this is possible until Level 1 integration happens at NCSA (see Document LDM-230, the operations concepts).

{\bf Solar System Processing -- } Since Solar System Processing takes \texttt{DIASources} as input, any Special Programs images that can be run through the Alert Pipeline can also be incorporated into Solar System Processing.
As discussed under "Solar System Objects (SSO)" in Appendix \ref{sec:prevpropsp}, most of the Special Programs data associated with SS science will obtain standard visit images anyway.
%There was some concern that a large number of small-separation sources might overwhelm the processing system (i.e., from a deep drilling field with many exposures in a sequence), but upon further consideration this worry was rejected.

\subsubsection{The Data Release Pipeline (DRP)}\label{ssec:dmplans_drp}

This document is not the place for a full consideration of whether or not it would be ``scientifically beneficial" to include any Special Programs data in the DRP data products -- namely, the deep image CoAdds and their corresponding {\tt Source} and {\tt Object} catalogs (\citeds{LSE-163}) -- and we leave that decision for the data quality assessment team in LSST Operations. One example might be when Special Programs data brings additional area up to the same level of depth and cadence as the rest of the WFD main survey. Another may be if including some or all of the shallower Galactic Plane coverage suppresses edge effects or low-order modes in the all-sky photometric solutions.


\clearpage
% % % % % % % % % % % % % % % % % % 
\section{Enabling the Discovery and Analysis of Special Programs Data Products}\label{sec:analysis}

\subsection{Anticipated Rubin Science Platform Capabilities for Special Programs }

\begin{itemize}
\item discoverability of SP data when browsing an all-sky map
\item to query data by tag of WFD, WFD mini-survey, or Special Program (e.g., DDF field, micro-survey identifier)
\end{itemize}





\clearpage
% % % % % % % % % % % % % % % % % %
\section{Special Programs Processing Examples}\label{sec:SPCS}

For further insight to the DM-related needs of potential Special Programs, we can write out all of the data acquisition and processing steps, in order, that some of the proposed Special Programs might use.
Note that we are not including any analysis in these descriptions, only processing and products. These are not necessarily complete and may even be incorrect in some places, as we are not experts in the science needs of these potential Special Programs; they could use some more thought and input.

Basic steps that we use to describe a processing case study: \\
Step 1. Data Acquisition. \\
Step 2. Inclusion in the Prompt Pipeline and Alert Generation. \\
Step 3. Delivery of LSST Processed Images. \\
Step 4. Reconfigured Processing Pipelines and Separate Data Products. \\
Step 5. Inclusion in the DRP Data Products for the WFD Main Survey. \\
Step 6. User-Generated Pipelines and Products. \\

\subsection{Searching for TNOs with Shift-and-Stack}\label{ssec:SPCS_TNO}

This Special Programs processing summary is based on Becker et al. (2011) white paper to find TNOs with shift-and stack (SAS) \citedsp{Document-11013}.

Step 1. Data Acquisition. \\
The observational sequence is triggered. In a single night, the 9 adjacent fields in a 3x3 grid are observed with $336$ $\times$ $15$ second $r$-band exposures. This sequence is always repeated 2-3 nights later. This re-visit sequence is repeated 3 more times: 1.5 months, 3 months, and 13.5 months later. Data obtained in the $g$-band filter is also acceptable. \citedsp{Document-11013}

Step 2. Inclusion in the Prompt Pipeline and Alert Generation. \\
Each $2\times15$ second visit is processed in the Prompt pipeline and Alerts are released within 60 seconds.

Step 3. Delivery of LSST Processed Images. \\
The raw, reduced, and calibrated exposures and difference images from the Prompt pipeline are made available within \texttt{L1PublicT} (currently 24 hours; LSR-REQ-0104), but this is not very relevant for this program, which requires a year of dispersed observations before the processing pipelines for SAS can be run.

Step 4. Reconfigured Processing Pipelines and Separate Data Products. \\
Shift-and-stack processing is beyond the scope of DM's algorithms.

Step 5. Inclusion in the DRP Data Products for the WFD Main Survey. \\
As with all Special Programs data, they might be included in the products of the WFD main survey if DM decides it is beneficial. However, since these images are much deeper than stacks made from the WFD survey, and the strict timing of the observations might lead to their acquisition in sub-optimal conditions, it is unlikely that they would \textit{all} be incorporated.

Step 6. User-Generated Pipelines and Products. \\
The user-generated pipeline running the shift-and-stack processing will be set up and submitted for batch processing by the user through the Science Platform or on an external processor. Pipeline inputs will be the 336 processed exposures per field per re-visit sequence. The DRP difference imaging routine will be used with the same template tract/patch for all. Custom, user-generated algorithms will shift the exposures and create difference images, then DRP routines can stack and do source detection and characterization and generate an object database. Custom code will derive orbital parameters for the detections and add them to a {\tt SSObjects}-like database.


%\input{spcs_sas}

\subsection{Searching for Supernovae in Deep Drilling Fields}\label{ssec:SPCS_SNDDF}

Step 1. Data Acquisition. \\
On a single deep drilling field, the scheduler obtains e.g., 5, 10, 10, 9, and 10 visits with $2\times15$ second exposures in $grizy$ (or similar for the night's filter set) and a small dither pattern between visits.

Step 2. Inclusion in the Prompt Pipeline and Alert Generation. \\
Each $2\times15$ second visit is processed by the Prompt pipeline's DIA, and Alerts are released within 60 seconds. They are flagged to denote the image source is a DDF and that source association might be compromised.

Step 3. Delivery of LSST Processed Images. \\
The raw, reduced, and calibrated exposures and difference images from the Prompt pipeline are made available within \texttt{L1PublicT} (currently 24 hours; LSR-REQ-0104).

Step 4. Reconfigured Processing Pipelines and Separate Data Products. \\
The required data products for this science goal can be met by reconfiguring the DM pipelines. First, a template image for the field will be made using DM stacking algorithms. On nights when this DDF is observed, at the end of the sequence of observations, DM algorithms are used to create a nightly deep stack, PSF-match it with the template, create a deep difference image, run source detection on the differences, and create separate databases of \texttt{DIAObject}, \texttt{DIASource}, and \texttt{Object} that are unique to this DDF. The LSST codes for alert packet and transport could be used to distribute the detected objects e.g., to the same brokers that receive the Alert Stream, or alternative destinations. However, these packets would not be distributed via the LSST {\tt Alert Stream}, and would need to be identified as, e.g., DDF Alerts.

Step 5. Inclusion in the DRP Data Products for the WFD Main Survey. \\
As with all Special Programs data, they might be included in the products of the WFD main survey if DM decides it is beneficial.

Step 6. User-Generated Pipelines and Products. \\
For the science goal of searching for supernovae in nightly stacked DDF images, no separate user-generated software appears necessary.


\subsection{A Twilight Survey with Short Exposures}\label{ssec:SPCS_Twilight}

Several kinds of twilight surveys with short exposures have been or might be proposed: to put brighter stars (or transients such as supernovae) that saturate in a $15$ second image onto the LSST photometric system and/or to observe the Sweetspot, 60 degrees from the sun, for near-Earth objects. The processing case study for these is currently limited by unknowns about the first step: the reduction of processed single visit images.

Step 1. Data Acquisition. \\
At a specified time (or e.g., 6 degree twilight), the scheduler begins dither pattern of short exposures. Location and exposure times are set by the sky brightness and desired saturation limits.

Step 2. Inclusion in the Prompt Pipeline and Alert Generation. \\
Pending studies of short-exposure suitability for DIA (see Section \ref{sec:procbounds}) and scalable processing capabilities to incorporate a faster image-input rate than $1$ every $30$ seconds, these data could {\it potentially} be incorporated and spawn Alerts.

Step 3. Delivery of LSST Processed Images. \\
Pending the issues mentioned above, the raw, reduced, and calibrated exposures and difference images from the Prompt pipeline are made available within  \texttt{L1PublicT} (currently 24 hours; LSR-REQ-0104).

Step 4. Reconfigured Processing Pipelines and Separate Data Products. \\
This is officially not determined, but so long as the short-exposure images can be processed and have enough stars for photometric and astrometric calibration, reconfigured DM pipelines will probably be sufficient for creating image and catalog products from this kind of data.

Step 5. Inclusion in the DRP Data Products for the WFD Main Survey. \\
These short-exposure, high sky background images would not contribute to the DRP data products created for the WFD survey.

Step 6. User-Generated Pipelines and Products. \\
If short-exposure images cannot be processed with the existing DM algorithms, a user-generated processing pipeline might be needed to reduce the raw data. 

Side note: A short-exposure survey of the bright stars of M67, described in Chapter 10.4 of the Observing Strategy White Paper \citep{2017arXiv170804058L}, suggests using the stretch goal of 0.1 second exposures or, if that is not possible, \textit{"custom pixel masks to accurately perform photometry on stars as much as 6 magnitudes brighter than the saturation level"}. This would be considered a user-generated algorithm.

\subsection{The Galactic Plane Survey for Variable Stars and/or Exoplanets}\label{ssec:SPCS_GPVSEx}

Step 1. Data Acquisition. \\
The schedule incorporates fields in the Galactic Plane, and executes $2\times15$ second visits in these fields (or shorter, for a shallower depth than the WFD main survey).

Step 2. Inclusion in the Prompt Pipeline and Alert Generation. \\
Each $2\times15$ second visit is processed in the Prompt pipeline and Alerts are released within 60 seconds. Extremely crowded fields might have to be skipped if they take longer to process and violate the $60$ second latency for Alerts. 

Step 3. Delivery of LSST Processed Images. \\
The raw, reduced, and calibrated exposures and difference images from the Prompt pipeline are made available within  \texttt{L1PublicT} (currently 24 hours; LSR-REQ-0104).

Step 4. Reconfigured Processing Pipelines and Separate Data Products. \\
The image and catalog products needed for science with the Galactic Plane are very similar to the products of the Prompt and DRP pipelines, so it seems that not much reconfiguration would be needed. The biggest difference might be the incorporation of a user-supplied deblender algorithm optimized for very crowded fields.

Step 5. Inclusion in the DRP Data Products for the WFD Main Survey. \\
It is quite likely that images from the Galactic Plane will be included into the products of the WFD main survey, as they could e.g., reduce edge effects and help with global photometric classification, but this will depend on deblender performance, and left to the discretion of DM. 

Step 6. User-Generated Pipelines and Products. \\
It seems likely that science users will want to deploy their alternative deblending algorithms on this data set and create their own catalogs.

\subsection{Gravitational Wave Event Follow-Up}\label{ssec:SPCS_GW}

For a description of how target of opportunity data to search for the optical counterparts of gravitational wave events would be processed, see \citeds{rtn-008}.


% % % % % % % % % % % % % % % % % %
\clearpage
\bibliography{local,lsst,refs,books,refs_ads}


% % % % % % % % % % % % % % % % % %
\clearpage
\appendix

\input{ap_docrev.tex}

\input{ap_hardbounds.tex}

\input{ap_procbounds.tex}

\input{ap_prevpropsp.tex}

\end{document}
