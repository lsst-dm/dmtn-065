\section{Hypothetical Scenarios}\label{sec:spcs}

\textbf{\emph{Hypothetical}} examples of Special Programs
and the standard, Special, and User-Generated processing
to illustrate what \emph{might be done}.

The details of the data acquisition and processing mentioned below are 
\emph{just illustrative examples} of decisions that have yet to be made.

The steps used to describe the hypothetical processing for each case scenario are: \\
Step 1. Data acquisition. \\
Step 2. Standard Prompt processing and Alert Production. \\
Step 3. Special Processing with reconfigured pipelines. \\
Step 4. Standard processing for inclusion in Main Survey data products. \\
Step 5. User-Generated Processing. \\

\subsection{Outer solar system mini-survey}\label{ssec:SPCS_TNO}

This hypothetical Special Programs processing summary is based on the Becker et al. (2011) 
white paper to find outer solar system objects with shift-and stack (SAS) \citedsp{Document-11013}.

Step 1. Data acquisition. \\
In a single night, the 9 adjacent fields in a 3x3 grid are observed with 
$336$ $\times$ $15$ second $r$ or $g$-band exposures (168 standard visits). 
These observations are repeated 2-3 nights later, and then this 2-night sequence
is repeated 3 more times: 1.5 months, 3 months, and 13.5 months later. 
They are not all at the same RA, Dec, but at selected ecliptic coordinates.

Step 2. Standard Prompt processing and Alert Production. \\
Each $2\times15$ second standard visit is processed by the Prompt pipeline 
and alerts are released within 60 seconds.
Within 24 hours, the {\tt DiaSource} and {\tt DiaObject} catalogs are updated
to include the results of Prompt processing of these visits.
After 80 hours, the processed visit images and difference images become available.
All images and sources originating from this Special Program have 
region and observing mode labels, e.g., ``SP-OSSO".

The results of Prompt processing are not very relevant for this Special Program's primary science goal,
which requires a year of dispersed observations before the processing pipelines for shift-and-stack can be run.
However, including these data in Prompt processing means that
they can contribute to LSST's other time-domain and Solar System science goals.

Step 3. Special Processing with reconfigured pipelines. \\
None possible. 
Shift-and-stack processing is beyond the scope of existing algorithms in the LSST Science Pipelines.

Step 4. Standard processing for inclusion in Main Survey data products. \\
Every year, each $2\times15$ second standard visit is reprocessed by the DIA data release pipelines
and the results are included alongside Main Survey data in the relevant DIA data products
(e.g., processed visit images, difference images, associated source catalogs).
In the first year after the Special Program is executed,
Rubin Data Management finds that 10\% of the standard visits from this Special Program
had coordinates and image quality that help improve uniformity of the all-sky coadd,
and so they are included.
In later years, this fraction decreases (remember, this is \emph{hypothetical}).
In all data releases, any and all processed images and catalog sources that originate in visits from this Special Program
have the same region and observing mode labels, e.g., ``SP-OSSO".

Step 5. User-Generated Processing. \\
The User-Generated Processing pipeline running the shift-and-stack processing is be set up and submitted 
for batch processing by the user through the Science Platform or on an external system. 
The pipeline's inputs are the processed visit images (and/or difference images) from Prompt processing.
User-generated custom algorithms then shift-and-stack the images, and then the LSST Science Pipelines
tasks are used to do source detection and characterization and create catalogs.
User-generated custom code derives orbital parameters for the detections, and stores
them in a user-generated catalog with a similar format to {\tt SSObjects}.


\subsection{Deep Drilling Field}\label{ssec:SPCS_SNDDF}

Step 1. Data acquisition. \\
In the COSMOS DDF, the scheduler obtains 10 standard visits in a row in each of the $griz$ filters
with a small dither pattern between visits.
This happens every other night for a three month season for four years.

Step 2. Standard Prompt processing and Alert Production. \\
Same as above, but the observing mode label might be, e.g., ``SP-DDF-COSMOS".

Step 3. Special Processing with reconfigured pipelines. \\
First, a template image of appropriate depth for ``nightly" difference imaging is created.
At the end of each nightly sequence of observations, a pipeline based on recongfigured 
components of the LSST Science Pipelines could be automatically triggered.
This pipeline creates nightly coadds in each filter and runs DIA using the template.
Alerts are \emph{not} produced, but unique and separate catalogs with the same format
as {\tt DiaObject} and {\tt DiaSource} could be updated within 24 hours (not a requirement).
Instead of running daily, this processing might run weekly instead (TBD).
At the end of each season, deeply coadded images that include all of the DDF's visits 
from all years are re-generated, along with a separate {\tt Object}-like catalog.
All images and catalogs are stored in separate butler collections and TAP tables from
the Main Survey data products.

Step 4. Standard processing for inclusion in Main Survey data products. \\
Every year, each standard visit is reprocessed by the DIA data release pipelines
and the results are included alongside Main Survey data in the relevant DIA data products
(e.g., processed visit images, difference images, associated source catalogs).
Due to their small dither and lack of rotation, not even a single DDF image 
is used to supplement the Main Survey's all-sky coadd.

Step 5. User-Generated Processing. \\
In order to achieve a secondary science goal of finding very high-$z$ faint supernovae,
a team of users reconfigure the LSST Science Pipelines to create weekly deep coadds
of the COSMOS field an appropriate-depth template image, and to run DIA at the
end of the season.
These data products are stored in separate catalogs with the same format and schema as
the {\tt DiaSource} and {\tt DiaObject} tables that are private to the team.


\subsection{Short-exposure twilight survey}\label{ssec:SPCS_Twilight}

Twilight observations obtained at, e.g., 60 degrees from the Sun, are particularly
well-suited for finding Near-Earth Objects (NEOs).

Step 1. Data acquisition. \\
At a specified time (or e.g., 6 degree twilight), the scheduler begins a dither pattern of 
$2$-second exposures. 
Coordinates and exposure times are set by the Sun distance, sky brightness, and desired saturation limits.

Step 2. Standard Prompt processing and Alert Production. \\
Pending studies of DIA and Alert Production pipeline capabilities to process 
short-exposure, high sky-background images (see Section~\ref{ssec:proc_bounds_processing}).

Step 3. Special Processing with reconfigured pipelines. \\
Pending studies of the LSST Science Pipelines capabilities to process 
short-exposure, high sky-background images (see Section~\ref{ssec:proc_bounds_processing}).

Step 4. Standard processing for inclusion in Main Survey data products. \\
These short-exposure, high sky background images would not contribute to the data products created for the Main Survey.

Step 5. User-Generated Processing. \\
If short-exposure images cannot be processed with the existing DM algorithms, 
user-generated processing would be needed to reduce the raw data, and to 
futher detect and characterize sources in the processed images.



